
cat aggregate.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat create_deliver_customer_attempts_test_data >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat customer_tasks_new.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat deliver_customer_attempts >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat ezcom_notifications.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat find.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat fix_fidelity >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat README.md >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat sqlInsertGenerator >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat subpoena.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat summary_discrepancy.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat translate_customer_attempt_csv.py >> ref.txt
echo "-----------------------================================>" >> ref.txt
cat udd.py >> ref.txt
echo "-----------------------================================>" >> ref.txt

#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv
import pickle
import csv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def cdrs(day, invoice, criteria):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match' : criteria},
        {'$group' : { '_id': 1, 'total' : {'$sum': '$rated.bill.amt'}}},
    ]

    res = c.aggregate(pipe)

    return res['result'][0]['total'] if len(res['result'])>0 else 0

def writeheader(writer):
    writer.writerow(dict((fn, fn) for fn in writer.fieldnames))


def run_search(month, subscriber):
    #import pdb; pdb.set_trace()

    file = open('results.csv', 'wb+')
    file.write('billrec.num.called' + ',')
    file.write('billrec.num.calling' + ',')
    file.write('udr.start.date' + ',')
    file.write('udr.start.time' + ',')
    file.write('udr.disc.date' + ',')
    file.write('udr.disc.time' + ',')
    file.write('udr.ani.nat' + ',')
    file.write('udr.lrn.nat' + ',')
    file.write('udr.ip.orig' + ',')
    file.write('udr.ip.term' + '\n')
    file.close()

    for day in month:
        get_day(day, subscriber)


def get_day(day, subscriber):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    #colname = 'merged_cdrs_20140806'
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match':{ '$or' : [ {'billrec.num.called':subscriber}, {'billrec.num.calling':subscriber}]}},
        {'$group':{ '_id':
                       {'billrec_num_called':'$billrec.num.called',
                       'billrec_num_calling':'$billrec.num.calling',
                       'udr_start_date':'$udr.start.date',
                       'udr_start_time':'$udr.start.time',
                       'udr_disc_date':'$udr.disc.date',
                       'udr_disc_time':'$udr.disc.time',
                       'udr_ani_nat':'$udr.ani.nat',
                       'udr_lrn_nat':'$udr.lrn.nat',
                       'udr_ip_orig':'$udr.ip.orig',
                       'udr_ip_term':'$udr.ip.term'
                       }}},
           ]

    docs = c.aggregate(pipe)

    file = open('results.csv', 'ab+')
    for item in docs['result']:
        file.write(str(item['_id']['billrec_num_called'])+ ',')
        file.write(str(item['_id']['billrec_num_calling']) + ',')
        file.write(str(item['_id']['udr_start_date']) + ',')
        file.write(str(item['_id']['udr_start_time']) + ',')
        file.write(str(item['_id']['udr_disc_date']) + ',')
        file.write(str(item['_id']['udr_disc_time']) + ',')
        file.write(str(item['_id']['udr_ani_nat']) + ',')
        file.write(str(item['_id']['udr_lrn_nat']) + ',')
        file.write(str(item['_id']['udr_ip_orig']) + ',')
        file.write(str(item['_id']['udr_ip_term']) + '\n')

    file.close()


if __name__ == '__main__':
    if len(argv) <= 3:
        print 'Usage: subpoena.py <start date yyyy-mm-dd> <end date yyyy-mm-dd> <subscriber#>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]
    subscriber = argv[3]

    run_search(month, subscriber)

-----------------------================================>
#!/bin/bash

# This script sets up a customer_csvs directory structure,
# populating it with dummy UDR data. It is intended to be
# run prior to running the 'deliver_customer_attempts' script
# for testing purposes.
# 
# 'deliver_customer_attempts' does three things:
# 
#  1. Removes any .csv files older than today's date minus
#     DAYS_TO_DELETE. This also includes any corresponding
#     .zip and marker files.
#
#  2. Creates a .zip file for any .csv files which don't
#     currently have them. It also creates a marker file,
#     which is a snapshot of the .csv file's current size,
#     and incorporates that size into the marker filename.
#     Eg. attempt_1003314_20140516.csv.filesize.203
#
#     When a .csv does have a .zip file, its current size
#     if checked against the marker file size. If they don't
#     match - ie. the .csv has grown - the .zip file is recreated.
# 
#  3. Pushes the .zip files to custftp.intelepeer.net via rsync.
# 
# 
# 

cd customer_udr
if [ $? -eq 0 ]; then
    echo "Changed directory to customer_cdr"
    echo "Creating test data"
    rm -rf *
    mkdir 10000385  10000388  10000415  1002460  1003271  1003314  1003902  1005607 1115555
else	
    echo "Couldn't change directory customer_udr. Exiting..."
    exit
fi

touch 10000385/attempt_10000385_20140407.csv
touch 10000385/attempt_10000385_20140408.csv
touch 10000385/attempt_10000385_20140409.csv
touch 10000385/attempt_10000385_20140410.csv
touch 10000385/attempt_10000385_20140507.csv
touch 10000385/attempt_10000385_20140508.csv
touch 10000385/attempt_10000385_20140509.csv
touch 10000385/attempt_10000385_20140510.csv
touch 10000385/attempt_10000385_20140511.csv
touch 10000385/attempt_10000385_20140512.csv
touch 10000385/attempt_10000385_20140513.csv
touch 10000385/attempt_10000385_20140514.csv
touch 10000385/attempt_10000385_20140515.csv
touch 10000385/attempt_10000385_20140516.csv

touch 10000388/attempt_10000388_20140512.csv
touch 10000388/attempt_10000388_20140513.csv
touch 10000388/attempt_10000388_20140514.csv
touch 10000388/attempt_10000388_20140515.csv
touch 10000388/attempt_10000388_20140516.csv

touch 10000415/attempt_10000415_20140512.csv
touch 10000415/attempt_10000415_20140513.csv
touch 10000415/attempt_10000415_20140514.csv
touch 10000415/attempt_10000415_20140515.csv
touch 10000415/attempt_10000415_20140516.csv

touch 1002460/attempt_1002460_20140512.csv
touch 1002460/attempt_1002460_20140513.csv
touch 1002460/attempt_1002460_20140514.csv
touch 1002460/attempt_1002460_20140515.csv
touch 1002460/attempt_1002460_20140516.csv

touch 1003271/attempt_1003271_20140512.csv
touch 1003271/attempt_1003271_20140513.csv
touch 1003271/attempt_1003271_20140514.csv
touch 1003271/attempt_1003271_20140515.csv
touch 1003271/attempt_1003271_20140516.csv

touch 1003314/attempt_1003314_20140512.csv
touch 1003314/attempt_1003314_20140513.csv
touch 1003314/attempt_1003314_20140514.csv
touch 1003314/attempt_1003314_20140515.csv
touch 1003314/attempt_1003314_20140516.csv
echo "some text some text some text some text" >> 1003314/attempt_1003314_20140512.csv
echo "some text some text some text some text some text" >> 1003314/attempt_1003314_20140513.csv
echo "some text some text some text some text some text" >> 1003314/attempt_1003314_20140514.csv
echo "some text some text some text some text some text some text banana" >> 1003314/attempt_1003314_20140515.csv
echo "some text some text some text some text some text some text some text some text some text some" >> 1003314/attempt_1003314_20140516.csv

touch 1003902/attempt_1003902_20140512.csv
touch 1003902/attempt_1003902_20140513.csv
touch 1003902/attempt_1003902_20140514.csv
touch 1003902/attempt_1003902_20140515.csv
touch 1003902/attempt_1003902_20140516.csv

touch 1005607/attempt_1005607_20140512.csv
touch 1005607/attempt_1005607_20140513.csv
touch 1005607/attempt_1005607_20140514.csv
touch 1005607/attempt_1005607_20140515.csv
touch 1005607/attempt_1005607_20140516.csv

echo "some text some text some text some text" >> 1005607/attempt_1005607_20140514.csv
echo "some text some text some text some text some text" >> 1005607/attempt_1005607_20140515.csv
echo "some text some text some text some text some text some text" >> 1005607/attempt_1005607_20140516.csv

# Generate the filesize marker files
FILES=$(find . -type f -name *.csv)

for f in $FILES
do
  filesize=$(stat -c%s $f)
  #echo "Size of $f is $filesize"

  nameWithFileSize=$f.filesize.$filesize
  touch $nameWithFileSize
  #echo "New filename: $filename"
done

# For testing purposes, insert some data into any .csv files than have zero length.
# Also, there could be some files that don't have zero length. I need the purge_customer_csvs to create
# .zip files for them too, which is why I'm re-setting the filesize to 0 for those cases.
# This will cause a mismatch between .csv and .fileisze.n, causing the snapshot to be updated and a .zip
# file created.
for f in $FILES
do
  filesize=$(stat -c%s $f)
  if [ $filesize -eq 0 ]
  then
    echo "Some text to make filesize non-zero" >> $f
  else
    rm $f.filesize.*
    touch $f.filesize.0
  fi
done

# Change the modifications times of some .csvs to a couple of months ago.
# These are the ones that should get deleted.
touch -t 201404041200 10000385/attempt_10000385_20140407.csv
touch -t 201404041200 10000385/attempt_10000385_20140408.csv
touch -t 201404041200 10000385/attempt_10000385_20140409.csv
touch -t 201404041200 10000385/attempt_10000385_20140410.csv
touch -t 201404041200 10000385/attempt_10000385_20140507.csv
touch -t 201404041200 10000385/attempt_10000385_20140508.csv
touch -t 201404041200 10000385/attempt_10000385_20140509.csv
touch -t 201404041200 10000385/attempt_10000385_20140510.csv
touch -t 201404041200 10000385/attempt_10000385_20140511.csv
touch -t 201404041200 10000385/attempt_10000385_20140512.csv
touch -t 201404041200 10000385/attempt_10000385_20140513.csv
touch -t 201404041200 1003902/attempt_1003902_20140512.csv
touch -t 201404041200 1003902/attempt_1003902_20140513.csv
touch -t 201404041200 1003902/attempt_1003902_20140514.csv
touch -t 201404041200 1003902/attempt_1003902_20140515.csv
touch -t 201404041200 1003902/attempt_1003902_20140516.csv
touch -t 201404041200 1005607/attempt_1005607_20140512.csv
touch -t 201404041200 1005607/attempt_1005607_20140513.csv

# Create some csvs that won't have a marker snapshot
touch 1005607/attempt_1005607_20140517.csv
touch 1005607/attempt_1005607_20140518.csv
touch 1005607/attempt_1005607_20140519.csv
touch 1005607/attempt_1005607_20140520.csv

# Increase csv size to be bigger than the marker file - this tests
# the scenario whereby the .csv has grown bigger than the snapshot file size.
echo "elephantelephantelephantelephantelephantelephantelephantelephantelephant" >> 1003314/attempt_1003314_20140514.csv
echo "lions tigers bears oh my" >> 1003314/attempt_1003314_20140515.csv
echo " alpha some text some text some text some text some text some text some text some text some text some omega" >> 1003314/attempt_1003314_20140516.csv

# 1115555 is a dummy account
cp 1003314/* 1115555

# Empty directories to test rsync'ing.
mkdir 1225555
mkdir 1225556
mkdir 1225557
-----------------------================================>

from dw.tasks import BaseTask

from dw.model.customer import *
from dw.utils.ssh import SCP
from /home/ageorge import customer_new

import logging, csv, datetime, os
import pdb

class LoadCustomerList(BaseTask):
    ONE_DAY = datetime.timedelta(days=1)
    FORMAT  = "AL_%m%d%Y.CSV"
    
    def run_task(self,*args,**kwargs):
        remote = kwargs.pop('remote')
        if remote:
            remote_host, remote_dir = remote.split(":",1)
            path = self.get_latest_file(remote_host,remote_dir,kwargs.get('local_dir','/tmp'))
        else:
            path = kwargs.pop('path')

        if not path:
            return

        self.log.info("Loading customer account list: %s" % (path))

        loader = CustomerListLoader(logger=self.log)
        count = loader.load(path)

        self.log.info("Loaded %d customer accounts" % (count))

    def get_latest_file(self,remote_host,remote_dir,local_dir):
        #check for latest file
        dt = datetime.datetime.now()
        scp = SCP(remote_host,'apps','/home/apps/.ssh/id_rsa')
        for i in range(30):
            fname = dt.strftime(self.FORMAT)
            #check for local file first
            local_file = os.path.join(local_dir,fname)
            if os.path.exists(local_file):
                self.log.info("Latest customer account list already loaded: %s" % (fname))
                return None
            try:
                scp.copy_from(os.path.join(remote_dir,fname),local_dir)
                return local_file
            except Exception:
                #file probably wasn't there, just continue
                pass
            
            dt -= self.ONE_DAY

        self.log.info("No customer account list found within last 30 days")
        return None

class CustomerListLoader(object):
    """
    Parse an ezcom AL file, create/update customer accounts
    """

    def __init__(self,logger=None):
        self.log = logger or logging.getLogger()


    def load(self,path):
        parser = EZComAccountListParser()
        source = os.path.basename(path)

        count = 0
        for row in parser.parse_file(path):
            ezcom_id = int(row['Customer Number'])
            cust = Customer.find_by_ezcom_id(ezcom_id)
            if cust:
                self.log.debug("Update existing customer: %s" % (cust))
                pass
            if not cust:
                cust = Customer(ezcom_id=ezcom_id)
                self.log.debug("Add new customer: %s" % (cust))

            pdb.set_trace()

            cust.name = row['Customer Name']
            cust.sap_id = row['SAP ID']
            cust.bill_freq = row['Bill Frequency']
            cust.email = row['Email']
            cust.aggregator = row['Aggregator ID']
            cust.source = source

            if cust.aggregator == "PEER":
                continue

            if not 'is_ucc' in cust:
                cust.is_ucc = cust.determine_is_ucc()

            cust.save()
            count += 1

        return count


class EZComAccountListParser(object):
    """
    Parse an ezcom customer account list file (AL_*.CSV)
    """

    def __init__(self):
        pass

    FIELDNAMES = ['Aggregator ID',
                  'Customer Number',
                  'Customer Name',
                  'SAP ID',
                  'Bill Frequency',
                  'Email']

    def parse_file(self,path):
        with open(path,'rU') as fh:
            for row in self.parse(fh):
                yield(row)

    def parse(self,fh):
        # Determine the CSV dialect, and whether headers are included
        # The CSV from EzCom doesn't currently contain field names, but it may
        # in the future
        sample = fh.read(5000)
        fh.seek(0)
        
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(sample)

        fieldnames = None
        #if not sniffer.has_header(sample): # sniffer header detection not accurate...
        if not 'Customer Name' in sample:
            fieldnames = self.FIELDNAMES

        reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
        for row in reader:
            for k in row.keys():
                row[k] = row[k].strip()
            yield(row)
        

class LoadCustomerServiceIDs(BaseTask):
    """
    Loads the CustomerServiceID mappings used by UDRs, from a CSV file.
    The CSV file is a dump from the TGDB, taken using:
    mysql> select * into outfile '/tmp/customer_service.csv' 
           fields terminated by ',' optionally enclosed by '"'
           lines terminated by '\n'
           from CustomerService;
    """

    def run_task(self,*args,**kwargs):
        import pdb; pdb.set_trace()
        path = kwargs.pop('path')

        self.log.info("Loading customer service IDs: %s" % (path))
        self.load_from_file(path)

    FIELD_NAMES = ['CustomerServiceID',
                   'CustomerID',
                   'ServiceName',
                   'Description',
                   'IsEhanced',
                   'BillingID',
                   'selectionMethod']

    def load_from_file(self,path):
        reader = csv.DictReader(open(path,'rU'),fieldnames=self.FIELD_NAMES)
        for row in reader:
            cust = Customer.find_by_sap_id(row['BillingID'])
            if not cust:
                cust = Customer(sap_id=row['BillingID'])
            cust['customer_service'] = {
                'service_id': row['CustomerServiceID'],
                'id': row['CustomerID'],
                'service_name': row['ServiceName'],
                'billing_id': row['BillingID'],
                }
            cust.save()


class LoadVendorServiceIDs(BaseTask):
    """
    Loads the VendorServiceID mappings used by UDRs, from a CSV file.
    The CSV file is a dump from the TGDB, taken using:
    mysql> select * into outfile '/tmp/vendor_service.csv' 
           fields terminated by ',' optionally enclosed by '"'
           lines terminated by '\n'
           from VendorService;
    """

    def run_task(self,*args,**kwargs):
        path = kwargs.pop('path')

        self.log.info("Loading vendor service IDs: %s" % (path))
        self.load_from_file(path)

    FIELD_NAMES = ['VendorServiceID',
                   'VendorID',
                   'ServiceName',
                   'BillingIp',
                   'IsBillingIpFake',
                   'Description',
                   'IsEnhanced',
                   'EffBegin',
                   'EffEnd',
                   'selectionMethod']

    def load_from_file(self,path):
        reader = csv.DictReader(open(path,'rU'),fieldnames=self.FIELD_NAMES)
        for row in reader:
            # ...
            pass

-----------------------================================>
#/bin/bash  -x

# 'deliver_customer_attempts' does three things:
#
#  1. Removes any .csv files older than today's date minus
#     DAYS_TO_DELETE. This also includes any corresponding
#     .zip and marker files.
#
#  2. Creates a .zip file for any .csv files which don't
#     currently have them. It also creates a marker file,
#     which is a snapshot of the .csv file's current size,
#     and incorporates that size into the marker filename.
#     Eg. attempt_1003314_20140516.csv.filesize.203
#
#     When a .csv does have a .zip file, its current size
#     is checked against the marker file size. If they don't
#     match - ie. the .csv has grown - the .zip file is recreated.
#     
#     The T7100 cause value - column 8 - is mapped to an ISUP code.
#
#  3. Pushes the .zip files to pri.custftp.intelepeer.net via rsync.

#/bin/bash  -x

# Change these values as necessary
path="/home/ageorge/customer_udr"
tmpfile="./.isup.tmp"
DAYS_TO_DELETE=+9
SSH_KEY=/home/ageorge/.ssh/cdr_publishing_key.pri

# Setup test data - comment out if not running tests.
# ./create_deliver_customer_attempts_test_data


# Generate a marker file which contains a snapshot of the current .csv's size.
updateFileSizeMarker()
{
    # echo "Updating marker for $1"
    if [ -f $1.filesize.* ]
    then
        rm $1.filesize.*
    fi

    filesize=$(stat -c%s $1)
    nameWithFileSize=$1.filesize.$filesize
    touch $nameWithFileSize
}


# Purge files that have aged-out.
purgeFiles()
{
    #
    # Delete any .csv files that are older than Y-days back.
    #
    DELETED_FILES=`find $path -name "*.csv" -type f -mtime $DAYS_TO_DELETE -print -delete`

    #
    # Delete the corresponding .filesize and .zip files.
    #
    for d in $DELETED_FILES
    do
        echo "Deleting $d"
        echo "Deleting $d.filesize.*"
        rm $d.filesize.*
        if [ -f $d.zip ]
        then
            echo "Deleting $d.zip"
            rm $d.zip
        fi
    done
}


# Create .zip files.
createZipFiles()
{
    #
    # Create a zip file for each .csv that needs one.
    #
    FILES_TO_ZIP=`find $path -name "*.csv" -type f -print`
    for z in $FILES_TO_ZIP
    do
        # Convert T7100 cause values to ISUP codes.
        # mapT7100ToISUP $z

        # Only create a .zip file if there isn't one already,
        # or if the csv has grown since the last .zip was created.
        currentCsvFileSize=$(stat -c%s $z)

        if [ -f $z.filesize.* ]
        then
            markerFileSize=$(echo $z.filesize.* | awk -F. '{print $4}')
        else
            markerFileSize=-1
        fi

        # echo "markerFileSize: $markerFileSize"
        # echo "currentCsvFileSize: $currentCsvFileSize"

        if [ $currentCsvFileSize -ne $markerFileSize ]
        then
            updateFileSizeMarker $z
            echo "file is $z"
            echo "output file is $z.TRANSLATED"
            python translate_customer_attempt_csv.py --inputfile=$z --outputfile=$z.TRANSLATED
            echo "Creating $z.zip"
            cat $z.TRANSLATED | zip > $z.zip
            #rm -f $z.TRANSLATED
        fi
    done
}

pushFilesToCustftp()
{
    echo "Pushing files to custftp"

    for dir in `ls "$path"`
    do
    if [ -d "$path/$dir" ]; then
        echo $dir
        # rsync -avz -e "ssh -i $SSH_KEY" $path/$dir/*.zip \
        #    apps@pri.custftp.intelepeer.net:/home/customers/by_sap_id/$dir
        if [ `ls $path/$dir | wc -l` -gt 0 ]
        then
            rsync -avz -e "ssh -i /home/ageorge/.ssh/cdr_publishing_key.pri" $path/$dir/*.zip \
                apps@pri.custftp.intelepeer.net:/home/customers/by_sap_id/$dir
        fi
    fi
    done

    # rsync -avz -e "ssh -i /home/ageorge/.ssh/cdr_publishing_key.pri" customer_udr/1115555/*.zip \
    #     apps@pri.custftp.intelepeer.net:/home/customers/by_sap_id/1115555
}



purgeFiles

createZipFiles

pushFilesToCustftp
 
-----------------------================================>

# EZ-COM notifications are CSV files delivered via FTP
# (currently to ossbilling1.dal01:/home/ezcom/notifications)

import os, sys, re, csv

from voex.native import ServiceError

from cdrPublishingService.errors import *


class EzcomNotification(object):
    def __init__(self):
        self.path = None
        self._rows = []


    def add_row(self,row):
        self._rows.append(row)

        
    def rows(self):
        #for i in self._rows:
        #    yield i
        return self._rows
            

    def count(self):
        return len(self._rows)
    
    
        
class InvoiceDetailReport(EzcomNotification):
    """ Class for parsing IDR files from EZ-COM """

    FIELDNAMES_V1 = ['Aggregator ID',
                     'Customer Number',
                     'Customer Name',
                     'SAP ID',
                     'Invoice #',
                     'Total Call Amount',
                     'Invoice Amount',
                     'Bill Frequency',
                     'Last Invoice Date',
                     'R8 Call Amount',
                     'IP Call Amount',
                     'IL Call Amount',
                     '2P Call Amount',
                     'MRC',
                     'Taxes',
                     'Total Invoice Minutes']

    # Newer format (which should include headers by default) introduced
    # additional call types:
    FIELDNAMES = ['Aggregator',
                  'Account#',
                  'Name',
                  'SAP ID',
                  'Invoice#',
                  'Call Amount',
                  'Invoice$',
                  'Bill Freq',
                  'Inv Date',
                  'R8 Call Amt',
                  'IP Call Amt',
                  'IL Call Amt',
                  '2P Call Amt',
                  'R9 Call Amt',
                  '3P Call Amt',
                  'FF Call Amt',
                  'Charges',
                  'Taxes',
                  'Minutes']
    
    def __init__(self):
        super(InvoiceDetailReport,self).__init__()
        self.batch_name = None
        self.is_v1 = False

        self.index_account_nr = {}
        self.index_sap_id = {}
        self.index_name = {}
        self.index_invoice_nr = {}
        
    def parse_file(self,path):
        self.path = path
        fh = open(path,'rU')
        self.parse(fh)

        self.set_batch_name_from_path(path)

    
    def parse(self,fh):
        # Determine the CSV dialect, and whether headers are included
        # The CSV from EzCom doesn't currently contain field names, but it may
        # in the future
        sample = fh.read(5000)
        fh.seek(0)
        
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(sample)

        fieldnames = None
        #if not sniffer.has_header(sample): # sniffer header detection not accurate...
        if not 'Aggregator' in sample:
            # If the IDR doesn't have a header row, it is most likely a V1 IDR:
            fieldnames = self.FIELDNAMES_V1
            self.is_v1 = True

        reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
        for row in reader:
            for k in row.keys():
                row[k] = row[k].strip()
            self.add_row(row)

        if self.is_v1:
            self.fix_v1_to_current()

        self.index()
        

    def fix_v1_to_current(self):
        """ Translate column names from V1 format to latest format, and add missing columns """
        for row in self.rows():
            row['Aggregator'] = row['Aggregator ID']
            row['Account#'] = row['Customer Number']
            row['Name'] = row['Customer Name']
            #row['SAP ID'] = row['SAP ID']
            row['Invoice#'] = row['Invoice #']
            row['Call Amount'] = row['Total Call Amount']
            row['Invoice$'] = row['Invoice Amount']
            row['Bill Freq'] = row['Bill Frequency']
            row['Inv Date'] = row['Last Invoice Date']
            row['R8 Call Amt'] = row['R8 Call Amount']
            row['IP Call Amt'] = row['IP Call Amount']
            row['IL Call Amt'] = row['IL Call Amount']
            row['2P Call Amt'] = row['2P Call Amount']
            row['R9 Call Amt'] = '0.0'
            row['3P Call Amt'] = '0.0'
            row['FF Call Amt'] = '0.0'
            row['Charges'] = row['MRC']
            #row['Taxes'] = row['Taxes']
            row['Minutes'] = row['Total Invoice Minutes']
            
            

    def set_batch_name(self,batch_name):
        self.batch_name = batch_name

        # Also add to any of the individual rows that have been loaded:
        for inv in self.rows():
            inv['Batch Name'] = batch_name


    def set_batch_name_from_path(self,path):
        batch_name, ext = os.path.splitext(os.path.basename(path))
        self.set_batch_name(batch_name)


    def index(self):
        """ Populate the indices """
        for inv in self.rows():
            self.index_account_nr[inv['Account#']] = inv
            self.index_sap_id[inv['SAP ID']] = inv
            self.index_name[inv['Name']] = inv
            self.index_invoice_nr[inv['Invoice#']] = inv


    def find_by_name(self,name):
        try: return self.index_name[name]
        except KeyError: return None

    def find_by_account_nr(self,account):
        try: return self.index_account_nr[account]
        except KeyError: return None

        
    class Supplemental(EzcomNotification):
        """ Supplemental invoice detail report (SDR) that goes along with an IDR.
            Each row of the SDR gives total calls, mou, and charges per call-type / sub-call-type.
            The IDR gives just the total amount per Call-type, so this is the detail underneath
            that """
        FIELDNAMES = ['Aggregator',
                      'Account#',
                      'Name',
                      'SAP ID',
                      'Invoice#',
                      'Call Type',
                      'Sub-Call Type',
                      'Total Calls',
                      'Total Minutes',
                      'Total Amount']

        def __init__(self,idr=None):
            super(InvoiceDetailReport.Supplemental,self).__init__()
            self.idr = idr

        def parse_file(self,path=None):
            if not path:
                path = self.determine_path_from_idr()
            self.path = path
            if os.path.exists(self.path):
                fh = open(path,'rU')
                self.parse(fh)
            elif self.idr.is_v1:
                # No SDR for old IDR formats
                pass
            else:
                raise ServiceError(Errors.MissingSDR,self.path)


        def determine_path_from_idr(self):
            """ Determine the filename to load given the IDR this SDR is tied to.
                For an IDR name 'IDR_xxx', the SDR will be 'SDR_xxx' """
            if self.idr and self.idr.path:
                path = self.idr.path
                dir = os.path.dirname(path)
                base = os.path.basename(path)
                if base.startswith('IDR_'):
                    return os.path.join(dir,'SDR_' + base[4:])
            raise ServiceError(Errors.MissingSDR,self.idr.path if self.idr else None)

    
        def parse(self,fh):
            # Determine the CSV dialect, and whether headers are included
            sample = fh.read(5000)
            fh.seek(0)
        
            sniffer = csv.Sniffer()
            dialect = sniffer.sniff(sample)

            fieldnames = None
            #if not sniffer.has_header(sample): # sniffer header detection not accurate...
            if not 'Aggregator' in sample:
                fieldnames = self.FIELDNAMES
            
            reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
            for row in reader:
                for k in row.keys():
                    row[k] = row[k].strip()
                self.add_row(row)

            if self.idr:
                self.associate_idr()
                self.summarize_call_types()
        

        def associate_idr(self):
            """ Match up the detail rows from this file with the higher-level summary from the IDR """
            for row in self.rows():
                self.associate_idr_row(row)


        def associate_idr_row(self,row):

            #import pdb; pdb.set_trace()

            inv = self.idr.find_by_name(row['Name'])
            if not inv:
                return

            row['Invoice'] = inv
            
            call_type = row['Call Type']
            sub_type = row['Sub-Call Type']

            print 'Invoice'


            if '%s Detail' % call_type in inv and sub_type in inv['%s Detail' % call_type]:
                #print "Pair Exists"
                totalAmount = row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Amount']
                totalCalls = row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Calls']
                totalMinutes = row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Minutes']
            else:
                #print "Pair Does Not exist"
                totalAmount = 0
                totalCalls = 0
                totalMinutes = 0


            if '%s Detail' % (call_type) in inv:
                print "Detail exists"
                detail = inv['%s Detail' % (call_type)]
            else:
                print "Detail doesn't exist"
                detail = inv['%s Detail' % (call_type)] = {}

            detail[row['Sub-Call Type']] = row

            row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Amount'] = totalAmount + float(row['Total Amount'])
            row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Calls'] = totalCalls + float(row['Total Calls'])
            row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Minutes'] = totalMinutes + float(row['Total Minutes'])

            #import pprint
            #pp = pprint.PrettyPrinter(indent=4)
            #pp.pprint(row)


        def summarize_call_types(self):
            """ The IDR gives total amount per call type. This method also adds
                total call count, and total mou, by summing up the sub-call types
                per call type """
            for row in self.rows():
                self.summarize_call_types_row(row)


        def summarize_call_types_row(self,row):
            try: inv = row['Invoice']
            except KeyError: return

            call_type = row['Call Type']
            sub_type = row['Sub-Call Type']

            row_calls = int(row['Total Calls'])
            try: inv['%s Call Count' % (call_type)] += row_calls
            except KeyError: inv['%s Call Count' % (call_type)] = row_calls
            
            row_mou = float(row['Total Minutes'])
            try: inv['%s Call Minutes' % (call_type)] += row_mou
            except KeyError: inv['%s Call Minutes' % (call_type)] = row_mou



class AccountList(EzcomNotification):
    """ Class for parsing AL files from EZ-COM """

    FIELDNAMES = ['Aggregator ID',
                  'Customer Number',
                  'Customer Name',
                  'SAP ID',
                  'Bill Frequency',
                  'Email']
    
    def parse_file(self,path):
        fh = open(path,'rU')
        self.parse(fh)

    def parse(self,fh):
        # Determine the CSV dialect, and whether headers are included
        # The CSV from EzCom doesn't currently contain field names, but it may
        # in the future
        sample = fh.read(5000)
        fh.seek(0)
        
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(sample)

        fieldnames = None
        #if not sniffer.has_header(sample): # sniffer header detection not accurate...
        if not 'Customer Name' in sample:
            fieldnames = self.FIELDNAMES

        reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
        for row in reader:
            for k in row.keys():
                row[k] = row[k].strip()
            self.add_row(row)
            
-----------------------================================>
    # This is the bit you want for doing a find, Julius
    dd =  { 'billrec.num.called': 1,
        'billrec.num.calling': 1,
        'udr.start.date': 1,
        'udr.start.time': 1,
        'udr.disc.date': 1,
        'udr.disc.time': 1,
        'udr.ani.nat': 1,
        'udr.lrn.nat': 1,
        'udr.ip.orig': 1,
        'udr.ip.term': 1,
        '_id': 0 }
    #curs = cl.merged_cdrs.merged_cdrs_20140806.find({'udr.ani.nat':"6172792393"},dd.keys())
    curs = c.find( { '$or' : [ {'billrec.num.called':'subscriber'}, {'billrec.num.calling':'subscriber'}]},dd.keys())

    with open('mycsvfile.csv','wb') as f:
        for item in curs:
            w = csv.writer(f)
            w.writerow(item.keys())
            w.writerow(item.values())
            w.writerow(str(item['udr']['ani']['nat']))
-----------------------================================>
#/bin/bash  -x

# 'fix_fidelity'

services="FIDELITY_VOICE_SERVICES_$(date +'%Y')_$(date +'%m')"
svcs_trz="FIDELITY_VOICE_SVCS_TRZ_$(date +'%Y')_$(date +'%m')"
data_trzr="FIDELITY_VOICE_AND_DATA_TRZR_$(date +'%Y')_$(date +'%m')"

echo $services
echo $svcs_trz
echo $data_trzr

if [ ! -d tmp ]; then
    echo "Directory tmp does not exist"
    mkdir tmp
fi

cd tmp
if [ $? -eq 0 ]; then
    echo "Changed directory to tmp"
    rm -rf *
else
    echo "Couldn't change directory to tmp. Exiting..."
    exit
fi

processFile()
{
    zipfile="../$1*.zip"
    echo "processFile $zipfile"
    if [ -f $zipfile ]; then
        echo "$zipfile exists" 
        cp $zipfile .
        unzip $1*.zip
        csv="$1*.csv"
        echo $csv
        /home/ageorge/bin/ensureDecimals.py $csv tmpfile.csv
        /home/ageorge/bin/unix2dos tmpfile.csv
        mv tmpfile.csv $csv
        #echo "zip $zipfile $csv"
        echo "zip $1*.zip $csv"
        conv_zipfile="$1*.zip"
        echo "conv_zipfile $conv_zipfile"
        zip $conv_zipfile $csv
        cp $conv_zipfile ..
        rm *.csv *.zip
    fi
}

processFile $services
processFile $svcs_trz
processFile $data_trzr

cd ..
rmdir tmp
-----------------------================================>
-----------------------================================>
-----------------------================================>
#/bin/bash  -x

inputfile="./Data.txt"
# tr -d '\r' < Data.txt > datafile

cat $inputfile | while read TrunkGroupID TrunkGroupName GatewayID; do
   echo "INSERT into foo (COL1, COL2, COL3) VALUES ('$TrunkGroupID', '$TrunkGroupName', '$GatewayID');" >> output
done
-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv
import pickle
import csv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def cdrs(day, invoice, criteria):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match' : criteria},
        {'$group' : { '_id': 1, 'total' : {'$sum': '$rated.bill.amt'}}},
    ]

    res = c.aggregate(pipe)

    return res['result'][0]['total'] if len(res['result'])>0 else 0

def writeheader(writer):
    writer.writerow(dict((fn, fn) for fn in writer.fieldnames))


def run_search(month, subscriber):
    #import pdb; pdb.set_trace()

    file = open('results.csv', 'wb+')
    file.write('billrec.num.called' + ',')
    file.write('billrec.num.calling' + ',')
    file.write('udr.start.date' + ',')
    file.write('udr.start.time' + ',')
    file.write('udr.disc.date' + ',')
    file.write('udr.disc.time' + ',')
    file.write('udr.ani.nat' + ',')
    file.write('udr.lrn.nat' + ',')
    file.write('udr.ip.orig' + ',')
    file.write('udr.ip.term' + '\n')
    file.close()

    for day in month:
        get_day(day, subscriber)


def get_day(day, subscriber):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    #colname = 'merged_cdrs_20140806'
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match':{ '$or' : [ {'billrec.num.called':subscriber}, {'billrec.num.calling':subscriber}]}},
        {'$group':{ '_id':
                       {'billrec_num_called':'$billrec.num.called',
                       'billrec_num_calling':'$billrec.num.calling',
                       'udr_start_date':'$udr.start.date',
                       'udr_start_time':'$udr.start.time',
                       'udr_disc_date':'$udr.disc.date',
                       'udr_disc_time':'$udr.disc.time',
                       'udr_ani_nat':'$udr.ani.nat',
                       'udr_lrn_nat':'$udr.lrn.nat',
                       'udr_ip_orig':'$udr.ip.orig',
                       'udr_ip_term':'$udr.ip.term'
                       }}},
           ]
    #pipe.append({'$match':{'udr.ani.nat':"6172792393"}})
    docs = c.aggregate(pipe)
    #docs = cl.merged_cdrs.merged_cdrs_20140806.aggregate(pipe)
    #i = iter(docs)
    #item = i.next()
    #item = i.next()

    #itemCursor = cl.merged_cdrs.tmp_agg_res.find(exhaust=True)
    #for item in itemCursor:
        #print item

    #dd =  { 'billrec.num.called': 1,
        #'billrec.num.calling': 1,
        #'udr.start.date': 1,
        #'udr.start.time': 1,
        #'udr.disc.date': 1,
        #'udr.disc.time': 1,
        #'udr.ani.nat': 1,
        #'udr.lrn.nat': 1,
        #'udr.ip.orig': 1,
        #'udr.ip.term': 1,
        #'_id': 0 }
    #curs = cl.merged_cdrs.merged_cdrs_20140806.find({'udr.ani.nat':"6172792393"},dd.keys())
    ##curs = c.find({'udr.ani.nat':"6172792393"},dd.keys())
    #db.inventory.find( { $or: [ { quantity: { $lt: 20 } }, { price: 10 } ] } )
    #curs = c.find( { '$or' : [ {'billrec.num.called':'subscriber'}, {'billrec.num.calling':'subscriber'}]},dd.keys())

    #with open('mycsvfile.csv','wb') as f:
        #for item in curs:
            #w = csv.writer(f)
            #w.writerow(item.keys())
            #w.writerow(item.values())
            #w.writerow(str(item['udr']['ani']['nat']))

    file = open('results.csv', 'ab+')
    for item in docs['result']:
        file.write(str(item['_id']['billrec_num_called'])+ ',')
        file.write(str(item['_id']['billrec_num_calling']) + ',')
        file.write(str(item['_id']['udr_start_date']) + ',')
        file.write(str(item['_id']['udr_start_time']) + ',')
        file.write(str(item['_id']['udr_disc_date']) + ',')
        file.write(str(item['_id']['udr_disc_time']) + ',')
        file.write(str(item['_id']['udr_ani_nat']) + ',')
        file.write(str(item['_id']['udr_lrn_nat']) + ',')
        file.write(str(item['_id']['udr_ip_orig']) + ',')
        file.write(str(item['_id']['udr_ip_term']) + '\n')

    file.close()


if __name__ == '__main__':
    if len(argv) <= 3:
        print 'Usage: subpoena.py <start date yyyy-mm-dd> <end date yyyy-mm-dd> <subscriber#>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]
    subscriber = argv[3]

    run_search(month, subscriber)

-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def cdrs(day, invoice, criteria):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match' : criteria},
        {'$group' : { '_id': 1, 'total' : {'$sum': '$rated.bill.amt'}}},
    ]

    res = c.aggregate(pipe)

    return res['result'][0]['total'] if len(res['result'])>0 else 0


def hasUdr(day, invoice):
    criteria = {'rated.bill.inv': invoice, 'has.u' : {'$gt': 0}}
    return cdrs(day,invoice,criteria)

def hasBillrec(day, invoice):
    criteria = {'rated.bill.inv': invoice, 'has.b' : {'$gt': 0}}
    return cdrs(day,invoice,criteria)

def full_cdrs(day, invoice):
    criteria = {'has.b':{'$gt':0},
                'has.u':{'$gt':0},
                'has.r':{'$gt':0},
                'rated.bill.inv': invoice}
    return cdrs(day,invoice,criteria)


def summary(day, invoice):
    c = cl.summaries.ezcom_invoices 
    criteria = {'invoice': invoice}
    res = c.find_one(criteria)
    dayStr = day.strftime('%Y-%m-%d')
    return res['dates'][dayStr]['revenue'] if res is not None and dayStr in res['dates'] else 0

def run_invoice(month, invoice):
    columns = [summary, hasUdr, hasBillrec, full_cdrs]
    def valueStr(values):
        def eq(a,b):
            try:
                return fabs(a-b) < 0.1
            except:
                return a!=b

        res = ''
        prev = None
        for i in columns:
            val = values[i]
            res += ' %s %10s' % ('<>' if prev and not eq(val,prev) else '  ', val)
            prev = val
        return res

    columnNames = dict((x,x.__name__) for x in columns)
    totals = dict((x,0) for x in columns)

    print 'From %s to %s:' % (month[0], month[-1])
    print '%10s' % 'Day', valueStr(columnNames) 

    for day in month:
        amounts = dict((x,x(day,invoice)) for x in columns)
        for i in totals:
            totals[i] += amounts[i]
        print day.strftime('%Y-%m-%d'), valueStr(amounts)

    print 'Totals:', valueStr(totals)




if __name__ == '__main__':
    if len(argv) <= 3:
        print 'Usage: summary-discrepancy.py <start date yyyy-mm-dd> <end date yyyy-mm-dd> <invoice#>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]
    invoice = int(argv[3])

    run_invoice(month, invoice)

-----------------------================================>
#!/opt/anaconda/bin/python
import os
import sys
import optparse
import shutil
import csv
from collections import namedtuple

AttemptCsvComponents = [
            "derivedStartDate",
            "derivedStartTime",
            "derivedDisconnectTime",
            "mCustomerServiceName",
            "mAniNational",
            "mDestNational",
            "mDialedNational",
            "mDisconnectReason",
            "mDisconnectInitiator",
            "derivedDisconnectDate" ]
AttemptCsvLineComponents = namedtuple('AttemptCsvLineComponents', ' '.join(AttemptCsvComponents))


class StaticFieldTranslator:
    
    DisconnectReasonDict = {
                            '400': '41',
                            '401': '21',
                            '402': '21',
                            '403': '21',
                            '404': '1',
                            '405': '63',
                            '406': '79',
                            '407': '21',
                            '408': '102',
                            '410': '22',
                            '413': '127',
                            '414': '127',
                            '415': '79',
                            '416': '127',
                            '420': '127',
                            '421': '127',
                            '423': '127',
                            '480': '18',
                            '481': '41',
                            '482': '25',
                            '483': '25',
                            '484': '28',
                            '485': '1',
                            '486': '17',
                            '487': '16',
                            '500': '41',
                            '501': '79',
                            '502': '38',
                            '503': '41',
                            '504': '102',
                            '505': '127',
                            '513': '127',
                            '600': '17',
                            '603': '21',
                            '604': '1',
                            '1000': '16' ,
                            '1001': '41',
                            '1002': '41',
                            '1003': '41',
                            '1004': '41',
                            '1005': '102',
                            '1006': '63',
                            '1007': '47',
                            '1008': '34',
                            '1009': '41',
                            '1010': '102',
                            '1011': '102',
                            '1012': '16',
                            '1013': '41',
                            '1014': '41',
                            '1015': '44',
                            '1016': '42',
                            '1017': '42',
                            '1018': '34',
                            '1019': '34',
                            '1020': '34',
                            '1021': '63',
                            '1022': '41',
                            '1023': '16',
                            '1024': '102',
                            '1025': '102',
                            '1026': '102',
                            '1027': '41',
                            '1028': '3',
                            '1029': '41',
                            '1030': '96',
                            '1031': '42',
                            '1032': '42',
                            '1033': '21',
                            '1034': '41',
                            '1035': '41',
                            '1036': '41',
                            '1037': '57',
                            '1038': '34',
                            '1039': '41',
                            '1040': '42',
                            '1041': '42',
                            '1042': '42',
                            '1043': '34',
                            '1044': '102',
                            '1045': '63',
                            '1046': '63',
                            '1047': '16',
                            '1048': '102',
                            '1049': '96',
                            '1050': '47',
                            '1051': '3',
                            '1052': '102',
                            '1053': '44',
                            '1054': '41',
                            '1055': '41',
                            '1056': '101',
                            '1057': '101',
                            '1058': '110',
                            '1059': '34',
                            '1060': '34',
                            '1061': '34',
                            '1062': '34',
                            '1063': '34',
                            '1064': '41'
                            } 
    
    @staticmethod
    def translate_all(components):
        ''' call all translators
            each translator returns a new AttemptCsvLineComponents instance -- this is inefficient
            at the cost of the namedtuple benefits
        '''
        result = StaticFieldTranslator.translate_mDisconnectReason(components)
        return result
    
    @staticmethod
    def translate_mDisconnectReason(components):
        '''translate the mDisconnectReason field, if applicable'''
        new_val = StaticFieldTranslator.DisconnectReasonDict.get(components.mDisconnectReason, components.mDisconnectReason)
        if new_val == components.mDisconnectReason:
            return components
        else:
            return components._replace(mDisconnectReason=new_val)
        
        
            
def ParseOpts():
    '''Parse the command line options'''
    parser = optparse.OptionParser()
    
    parser.add_option("--inputfile", dest="inputfile", help="csv file to translate")
    parser.add_option("--outputfile", dest="outputfile", help="destination for resulting translated file")
        
    opts, args = parser.parse_args()
    return opts, args
    
    
def main():
    opts, args = ParseOpts()
    
    if opts.inputfile is None or opts.outputfile is None:
        print 'ERROR: empty arguments found[%s]' % (opts,)
    else:
        input_file_copy_filename = opts.inputfile + '.TMP'
        try:
            #operate on a copy of the file since the existing file might still be being written to
            shutil.copy(opts.inputfile, input_file_copy_filename)
            
            with open(input_file_copy_filename, 'rb') as inputcsvfile:
                csvreader = csv.reader(inputcsvfile)
                
                with open(opts.outputfile, 'w') as outputcsvfile:
                    outputcsvwriter = csv.writer(outputcsvfile, quoting=csv.QUOTE_ALL)
                    
                    for row in csvreader:
                        try:
                            attemptComponents = AttemptCsvLineComponents(*row)
                            attemptComponents = StaticFieldTranslator.translate_all(attemptComponents)
                            outputcsvwriter.writerow(attemptComponents)
                        except TypeError:
                            #print any parse errors, but continue regardless
                            print ('WARNING: invalid attempt line read into AttemptCsvLineComponents namedtuple:' +
                                '\n\t%s' +
                                '\n\tEither the file was not fully written, or an invalid file format is being read') % (row, )    
        except:
            print "Unexpected error [%s]" % (sys.exc_info()[0],)
            raise                            
        finally:
            if os.path.isfile(input_file_copy_filename):
                os.remove(input_file_copy_filename)
        
              

if __name__ == '__main__':
    main()
-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv
import pickle
import csv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def run_search(month):
    #import pdb; pdb.set_trace()

    #
    # Set the output file name, and write the column headers.
    #
    file = open('julius.csv', 'wb+')
    file.write('udr.pkg' + ',')
    file.write('billrec.cust' + ',')
    file.write('udr.gw' + ',')
    file.write('udr.tg.out' + ',')
    file.write('udr.file' + ',')
    file.write('udr.vend.name' + ',')
    file.write('udr.cust.name' + ',')
    file.write('udr.dial.nat' + '\n')
    file.close()

    for day in month:
        get_day(day)


def get_day(day):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    #colname = 'merged_cdrs_20140806'
    c = cl.merged_cdrs[colname]


    #
    # This is the data structure that maps to the result of your query.
    #
    dd = { 'udr.pkg':1,
        'billrec.cust':1,
        'udr.gw':1,
        'udr.tg.out':1,
        'udr.file':1,
        'udr.vend.name':1,
        'udr.cust.name':1,
        'udr.dial.nat':1 }

    #
    # This is the Mongo query.
    #
    curs = c.find({'udr.prod.code':"DD", 'udr.pkg':""}, dd.keys())

    #
    # You get a cursor back, and can just iterate through,
    # writing to the csv file.
    #
    file = open('julius.csv', 'ab+')
    for item in curs:
        file.write(str(item['udr']['pkg'])+ ',')
        file.write(str(item['billrec']['cust']) + ',')
        file.write(str(item['udr']['gw']) + ',')
        file.write(str(item['udr']['tg']['out']) + ',')
        file.write(str(item['udr']['file']) + ',')
        file.write(str(item['udr']['vend']['name']) + ',')
        file.write(str(item['udr']['cust']['name']) + ',')
        file.write(str(item['udr']['dial']['nat']) + '\n')

    file.close()


if __name__ == '__main__':
    if len(argv) <= 2:
        print 'Usage: udd.py <start date yyyy-mm-dd> <end date yyyy-mm-dd>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]

    run_search(month)

-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv
import pickle
import csv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def cdrs(day, invoice, criteria):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match' : criteria},
        {'$group' : { '_id': 1, 'total' : {'$sum': '$rated.bill.amt'}}},
    ]

    res = c.aggregate(pipe)

    return res['result'][0]['total'] if len(res['result'])>0 else 0

def writeheader(writer):
    writer.writerow(dict((fn, fn) for fn in writer.fieldnames))


def run_search(month, subscriber):
    #import pdb; pdb.set_trace()

    file = open('results.csv', 'wb+')
    file.write('billrec.num.called' + ',')
    file.write('billrec.num.calling' + ',')
    file.write('udr.start.date' + ',')
    file.write('udr.start.time' + ',')
    file.write('udr.disc.date' + ',')
    file.write('udr.disc.time' + ',')
    file.write('udr.ani.nat' + ',')
    file.write('udr.lrn.nat' + ',')
    file.write('udr.ip.orig' + ',')
    file.write('udr.ip.term' + '\n')
    file.close()

    for day in month:
        get_day(day, subscriber)


def get_day(day, subscriber):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    #colname = 'merged_cdrs_20140806'
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match':{ '$or' : [ {'billrec.num.called':subscriber}, {'billrec.num.calling':subscriber}]}},
        {'$group':{ '_id':
                       {'billrec_num_called':'$billrec.num.called',
                       'billrec_num_calling':'$billrec.num.calling',
                       'udr_start_date':'$udr.start.date',
                       'udr_start_time':'$udr.start.time',
                       'udr_disc_date':'$udr.disc.date',
                       'udr_disc_time':'$udr.disc.time',
                       'udr_ani_nat':'$udr.ani.nat',
                       'udr_lrn_nat':'$udr.lrn.nat',
                       'udr_ip_orig':'$udr.ip.orig',
                       'udr_ip_term':'$udr.ip.term'
                       }}},
           ]

    docs = c.aggregate(pipe)

    file = open('results.csv', 'ab+')
    for item in docs['result']:
        file.write(str(item['_id']['billrec_num_called'])+ ',')
        file.write(str(item['_id']['billrec_num_calling']) + ',')
        file.write(str(item['_id']['udr_start_date']) + ',')
        file.write(str(item['_id']['udr_start_time']) + ',')
        file.write(str(item['_id']['udr_disc_date']) + ',')
        file.write(str(item['_id']['udr_disc_time']) + ',')
        file.write(str(item['_id']['udr_ani_nat']) + ',')
        file.write(str(item['_id']['udr_lrn_nat']) + ',')
        file.write(str(item['_id']['udr_ip_orig']) + ',')
        file.write(str(item['_id']['udr_ip_term']) + '\n')

    file.close()


if __name__ == '__main__':
    if len(argv) <= 3:
        print 'Usage: subpoena.py <start date yyyy-mm-dd> <end date yyyy-mm-dd> <subscriber#>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]
    subscriber = argv[3]

    run_search(month, subscriber)

-----------------------================================>
#!/bin/bash

# This script sets up a customer_csvs directory structure,
# populating it with dummy UDR data. It is intended to be
# run prior to running the 'deliver_customer_attempts' script
# for testing purposes.
# 
# 'deliver_customer_attempts' does three things:
# 
#  1. Removes any .csv files older than today's date minus
#     DAYS_TO_DELETE. This also includes any corresponding
#     .zip and marker files.
#
#  2. Creates a .zip file for any .csv files which don't
#     currently have them. It also creates a marker file,
#     which is a snapshot of the .csv file's current size,
#     and incorporates that size into the marker filename.
#     Eg. attempt_1003314_20140516.csv.filesize.203
#
#     When a .csv does have a .zip file, its current size
#     if checked against the marker file size. If they don't
#     match - ie. the .csv has grown - the .zip file is recreated.
# 
#  3. Pushes the .zip files to custftp.intelepeer.net via rsync.
# 
# 
# 

cd customer_udr
if [ $? -eq 0 ]; then
    echo "Changed directory to customer_cdr"
    echo "Creating test data"
    rm -rf *
    mkdir 10000385  10000388  10000415  1002460  1003271  1003314  1003902  1005607 1115555
else	
    echo "Couldn't change directory customer_udr. Exiting..."
    exit
fi

touch 10000385/attempt_10000385_20140407.csv
touch 10000385/attempt_10000385_20140408.csv
touch 10000385/attempt_10000385_20140409.csv
touch 10000385/attempt_10000385_20140410.csv
touch 10000385/attempt_10000385_20140507.csv
touch 10000385/attempt_10000385_20140508.csv
touch 10000385/attempt_10000385_20140509.csv
touch 10000385/attempt_10000385_20140510.csv
touch 10000385/attempt_10000385_20140511.csv
touch 10000385/attempt_10000385_20140512.csv
touch 10000385/attempt_10000385_20140513.csv
touch 10000385/attempt_10000385_20140514.csv
touch 10000385/attempt_10000385_20140515.csv
touch 10000385/attempt_10000385_20140516.csv

touch 10000388/attempt_10000388_20140512.csv
touch 10000388/attempt_10000388_20140513.csv
touch 10000388/attempt_10000388_20140514.csv
touch 10000388/attempt_10000388_20140515.csv
touch 10000388/attempt_10000388_20140516.csv

touch 10000415/attempt_10000415_20140512.csv
touch 10000415/attempt_10000415_20140513.csv
touch 10000415/attempt_10000415_20140514.csv
touch 10000415/attempt_10000415_20140515.csv
touch 10000415/attempt_10000415_20140516.csv

touch 1002460/attempt_1002460_20140512.csv
touch 1002460/attempt_1002460_20140513.csv
touch 1002460/attempt_1002460_20140514.csv
touch 1002460/attempt_1002460_20140515.csv
touch 1002460/attempt_1002460_20140516.csv

touch 1003271/attempt_1003271_20140512.csv
touch 1003271/attempt_1003271_20140513.csv
touch 1003271/attempt_1003271_20140514.csv
touch 1003271/attempt_1003271_20140515.csv
touch 1003271/attempt_1003271_20140516.csv

touch 1003314/attempt_1003314_20140512.csv
touch 1003314/attempt_1003314_20140513.csv
touch 1003314/attempt_1003314_20140514.csv
touch 1003314/attempt_1003314_20140515.csv
touch 1003314/attempt_1003314_20140516.csv
echo "some text some text some text some text" >> 1003314/attempt_1003314_20140512.csv
echo "some text some text some text some text some text" >> 1003314/attempt_1003314_20140513.csv
echo "some text some text some text some text some text" >> 1003314/attempt_1003314_20140514.csv
echo "some text some text some text some text some text some text banana" >> 1003314/attempt_1003314_20140515.csv
echo "some text some text some text some text some text some text some text some text some text some" >> 1003314/attempt_1003314_20140516.csv

touch 1003902/attempt_1003902_20140512.csv
touch 1003902/attempt_1003902_20140513.csv
touch 1003902/attempt_1003902_20140514.csv
touch 1003902/attempt_1003902_20140515.csv
touch 1003902/attempt_1003902_20140516.csv

touch 1005607/attempt_1005607_20140512.csv
touch 1005607/attempt_1005607_20140513.csv
touch 1005607/attempt_1005607_20140514.csv
touch 1005607/attempt_1005607_20140515.csv
touch 1005607/attempt_1005607_20140516.csv

echo "some text some text some text some text" >> 1005607/attempt_1005607_20140514.csv
echo "some text some text some text some text some text" >> 1005607/attempt_1005607_20140515.csv
echo "some text some text some text some text some text some text" >> 1005607/attempt_1005607_20140516.csv

# Generate the filesize marker files
FILES=$(find . -type f -name *.csv)

for f in $FILES
do
  filesize=$(stat -c%s $f)
  #echo "Size of $f is $filesize"

  nameWithFileSize=$f.filesize.$filesize
  touch $nameWithFileSize
  #echo "New filename: $filename"
done

# For testing purposes, insert some data into any .csv files than have zero length.
# Also, there could be some files that don't have zero length. I need the purge_customer_csvs to create
# .zip files for them too, which is why I'm re-setting the filesize to 0 for those cases.
# This will cause a mismatch between .csv and .fileisze.n, causing the snapshot to be updated and a .zip
# file created.
for f in $FILES
do
  filesize=$(stat -c%s $f)
  if [ $filesize -eq 0 ]
  then
    echo "Some text to make filesize non-zero" >> $f
  else
    rm $f.filesize.*
    touch $f.filesize.0
  fi
done

# Change the modifications times of some .csvs to a couple of months ago.
# These are the ones that should get deleted.
touch -t 201404041200 10000385/attempt_10000385_20140407.csv
touch -t 201404041200 10000385/attempt_10000385_20140408.csv
touch -t 201404041200 10000385/attempt_10000385_20140409.csv
touch -t 201404041200 10000385/attempt_10000385_20140410.csv
touch -t 201404041200 10000385/attempt_10000385_20140507.csv
touch -t 201404041200 10000385/attempt_10000385_20140508.csv
touch -t 201404041200 10000385/attempt_10000385_20140509.csv
touch -t 201404041200 10000385/attempt_10000385_20140510.csv
touch -t 201404041200 10000385/attempt_10000385_20140511.csv
touch -t 201404041200 10000385/attempt_10000385_20140512.csv
touch -t 201404041200 10000385/attempt_10000385_20140513.csv
touch -t 201404041200 1003902/attempt_1003902_20140512.csv
touch -t 201404041200 1003902/attempt_1003902_20140513.csv
touch -t 201404041200 1003902/attempt_1003902_20140514.csv
touch -t 201404041200 1003902/attempt_1003902_20140515.csv
touch -t 201404041200 1003902/attempt_1003902_20140516.csv
touch -t 201404041200 1005607/attempt_1005607_20140512.csv
touch -t 201404041200 1005607/attempt_1005607_20140513.csv

# Create some csvs that won't have a marker snapshot
touch 1005607/attempt_1005607_20140517.csv
touch 1005607/attempt_1005607_20140518.csv
touch 1005607/attempt_1005607_20140519.csv
touch 1005607/attempt_1005607_20140520.csv

# Increase csv size to be bigger than the marker file - this tests
# the scenario whereby the .csv has grown bigger than the snapshot file size.
echo "elephantelephantelephantelephantelephantelephantelephantelephantelephant" >> 1003314/attempt_1003314_20140514.csv
echo "lions tigers bears oh my" >> 1003314/attempt_1003314_20140515.csv
echo " alpha some text some text some text some text some text some text some text some text some text some omega" >> 1003314/attempt_1003314_20140516.csv

# 1115555 is a dummy account
cp 1003314/* 1115555

# Empty directories to test rsync'ing.
mkdir 1225555
mkdir 1225556
mkdir 1225557
-----------------------================================>

from dw.tasks import BaseTask

from dw.model.customer import *
from dw.utils.ssh import SCP
from /home/ageorge import customer_new

import logging, csv, datetime, os
import pdb

class LoadCustomerList(BaseTask):
    ONE_DAY = datetime.timedelta(days=1)
    FORMAT  = "AL_%m%d%Y.CSV"
    
    def run_task(self,*args,**kwargs):
        remote = kwargs.pop('remote')
        if remote:
            remote_host, remote_dir = remote.split(":",1)
            path = self.get_latest_file(remote_host,remote_dir,kwargs.get('local_dir','/tmp'))
        else:
            path = kwargs.pop('path')

        if not path:
            return

        self.log.info("Loading customer account list: %s" % (path))

        loader = CustomerListLoader(logger=self.log)
        count = loader.load(path)

        self.log.info("Loaded %d customer accounts" % (count))

    def get_latest_file(self,remote_host,remote_dir,local_dir):
        #check for latest file
        dt = datetime.datetime.now()
        scp = SCP(remote_host,'apps','/home/apps/.ssh/id_rsa')
        for i in range(30):
            fname = dt.strftime(self.FORMAT)
            #check for local file first
            local_file = os.path.join(local_dir,fname)
            if os.path.exists(local_file):
                self.log.info("Latest customer account list already loaded: %s" % (fname))
                return None
            try:
                scp.copy_from(os.path.join(remote_dir,fname),local_dir)
                return local_file
            except Exception:
                #file probably wasn't there, just continue
                pass
            
            dt -= self.ONE_DAY

        self.log.info("No customer account list found within last 30 days")
        return None

class CustomerListLoader(object):
    """
    Parse an ezcom AL file, create/update customer accounts
    """

    def __init__(self,logger=None):
        self.log = logger or logging.getLogger()


    def load(self,path):
        parser = EZComAccountListParser()
        source = os.path.basename(path)

        count = 0
        for row in parser.parse_file(path):
            ezcom_id = int(row['Customer Number'])
            cust = Customer.find_by_ezcom_id(ezcom_id)
            if cust:
                self.log.debug("Update existing customer: %s" % (cust))
                pass
            if not cust:
                cust = Customer(ezcom_id=ezcom_id)
                self.log.debug("Add new customer: %s" % (cust))

            pdb.set_trace()

            cust.name = row['Customer Name']
            cust.sap_id = row['SAP ID']
            cust.bill_freq = row['Bill Frequency']
            cust.email = row['Email']
            cust.aggregator = row['Aggregator ID']
            cust.source = source

            if cust.aggregator == "PEER":
                continue

            if not 'is_ucc' in cust:
                cust.is_ucc = cust.determine_is_ucc()

            cust.save()
            count += 1

        return count


class EZComAccountListParser(object):
    """
    Parse an ezcom customer account list file (AL_*.CSV)
    """

    def __init__(self):
        pass

    FIELDNAMES = ['Aggregator ID',
                  'Customer Number',
                  'Customer Name',
                  'SAP ID',
                  'Bill Frequency',
                  'Email']

    def parse_file(self,path):
        with open(path,'rU') as fh:
            for row in self.parse(fh):
                yield(row)

    def parse(self,fh):
        # Determine the CSV dialect, and whether headers are included
        # The CSV from EzCom doesn't currently contain field names, but it may
        # in the future
        sample = fh.read(5000)
        fh.seek(0)
        
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(sample)

        fieldnames = None
        #if not sniffer.has_header(sample): # sniffer header detection not accurate...
        if not 'Customer Name' in sample:
            fieldnames = self.FIELDNAMES

        reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
        for row in reader:
            for k in row.keys():
                row[k] = row[k].strip()
            yield(row)
        

class LoadCustomerServiceIDs(BaseTask):
    """
    Loads the CustomerServiceID mappings used by UDRs, from a CSV file.
    The CSV file is a dump from the TGDB, taken using:
    mysql> select * into outfile '/tmp/customer_service.csv' 
           fields terminated by ',' optionally enclosed by '"'
           lines terminated by '\n'
           from CustomerService;
    """

    def run_task(self,*args,**kwargs):
        import pdb; pdb.set_trace()
        path = kwargs.pop('path')

        self.log.info("Loading customer service IDs: %s" % (path))
        self.load_from_file(path)

    FIELD_NAMES = ['CustomerServiceID',
                   'CustomerID',
                   'ServiceName',
                   'Description',
                   'IsEhanced',
                   'BillingID',
                   'selectionMethod']

    def load_from_file(self,path):
        reader = csv.DictReader(open(path,'rU'),fieldnames=self.FIELD_NAMES)
        for row in reader:
            cust = Customer.find_by_sap_id(row['BillingID'])
            if not cust:
                cust = Customer(sap_id=row['BillingID'])
            cust['customer_service'] = {
                'service_id': row['CustomerServiceID'],
                'id': row['CustomerID'],
                'service_name': row['ServiceName'],
                'billing_id': row['BillingID'],
                }
            cust.save()


class LoadVendorServiceIDs(BaseTask):
    """
    Loads the VendorServiceID mappings used by UDRs, from a CSV file.
    The CSV file is a dump from the TGDB, taken using:
    mysql> select * into outfile '/tmp/vendor_service.csv' 
           fields terminated by ',' optionally enclosed by '"'
           lines terminated by '\n'
           from VendorService;
    """

    def run_task(self,*args,**kwargs):
        path = kwargs.pop('path')

        self.log.info("Loading vendor service IDs: %s" % (path))
        self.load_from_file(path)

    FIELD_NAMES = ['VendorServiceID',
                   'VendorID',
                   'ServiceName',
                   'BillingIp',
                   'IsBillingIpFake',
                   'Description',
                   'IsEnhanced',
                   'EffBegin',
                   'EffEnd',
                   'selectionMethod']

    def load_from_file(self,path):
        reader = csv.DictReader(open(path,'rU'),fieldnames=self.FIELD_NAMES)
        for row in reader:
            # ...
            pass

-----------------------================================>
#/bin/bash  -x

# 'deliver_customer_attempts' does three things:
#
#  1. Removes any .csv files older than today's date minus
#     DAYS_TO_DELETE. This also includes any corresponding
#     .zip and marker files.
#
#  2. Creates a .zip file for any .csv files which don't
#     currently have them. It also creates a marker file,
#     which is a snapshot of the .csv file's current size,
#     and incorporates that size into the marker filename.
#     Eg. attempt_1003314_20140516.csv.filesize.203
#
#     When a .csv does have a .zip file, its current size
#     is checked against the marker file size. If they don't
#     match - ie. the .csv has grown - the .zip file is recreated.
#     
#     The T7100 cause value - column 8 - is mapped to an ISUP code.
#
#  3. Pushes the .zip files to pri.custftp.intelepeer.net via rsync.

#/bin/bash  -x

# Change these values as necessary
path="/home/ageorge/customer_udr"
tmpfile="./.isup.tmp"
DAYS_TO_DELETE=+9
SSH_KEY=/home/ageorge/.ssh/cdr_publishing_key.pri

# Setup test data - comment out if not running tests.
# ./create_deliver_customer_attempts_test_data


# Generate a marker file which contains a snapshot of the current .csv's size.
updateFileSizeMarker()
{
    # echo "Updating marker for $1"
    if [ -f $1.filesize.* ]
    then
        rm $1.filesize.*
    fi

    filesize=$(stat -c%s $1)
    nameWithFileSize=$1.filesize.$filesize
    touch $nameWithFileSize
}


# Purge files that have aged-out.
purgeFiles()
{
    #
    # Delete any .csv files that are older than Y-days back.
    #
    DELETED_FILES=`find $path -name "*.csv" -type f -mtime $DAYS_TO_DELETE -print -delete`

    #
    # Delete the corresponding .filesize and .zip files.
    #
    for d in $DELETED_FILES
    do
        echo "Deleting $d"
        echo "Deleting $d.filesize.*"
        rm $d.filesize.*
        if [ -f $d.zip ]
        then
            echo "Deleting $d.zip"
            rm $d.zip
        fi
    done
}


# Create .zip files.
createZipFiles()
{
    #
    # Create a zip file for each .csv that needs one.
    #
    FILES_TO_ZIP=`find $path -name "*.csv" -type f -print`
    for z in $FILES_TO_ZIP
    do
        # Convert T7100 cause values to ISUP codes.
        # mapT7100ToISUP $z

        # Only create a .zip file if there isn't one already,
        # or if the csv has grown since the last .zip was created.
        currentCsvFileSize=$(stat -c%s $z)

        if [ -f $z.filesize.* ]
        then
            markerFileSize=$(echo $z.filesize.* | awk -F. '{print $4}')
        else
            markerFileSize=-1
        fi

        # echo "markerFileSize: $markerFileSize"
        # echo "currentCsvFileSize: $currentCsvFileSize"

        if [ $currentCsvFileSize -ne $markerFileSize ]
        then
            updateFileSizeMarker $z
            echo "file is $z"
            echo "output file is $z.TRANSLATED"
            python translate_customer_attempt_csv.py --inputfile=$z --outputfile=$z.TRANSLATED
            echo "Creating $z.zip"
            cat $z.TRANSLATED | zip > $z.zip
            #rm -f $z.TRANSLATED
        fi
    done
}

pushFilesToCustftp()
{
    echo "Pushing files to custftp"

    for dir in `ls "$path"`
    do
    if [ -d "$path/$dir" ]; then
        echo $dir
        # rsync -avz -e "ssh -i $SSH_KEY" $path/$dir/*.zip \
        #    apps@pri.custftp.intelepeer.net:/home/customers/by_sap_id/$dir
        if [ `ls $path/$dir | wc -l` -gt 0 ]
        then
            rsync -avz -e "ssh -i /home/ageorge/.ssh/cdr_publishing_key.pri" $path/$dir/*.zip \
                apps@pri.custftp.intelepeer.net:/home/customers/by_sap_id/$dir
        fi
    fi
    done

    # rsync -avz -e "ssh -i /home/ageorge/.ssh/cdr_publishing_key.pri" customer_udr/1115555/*.zip \
    #     apps@pri.custftp.intelepeer.net:/home/customers/by_sap_id/1115555
}



purgeFiles

createZipFiles

pushFilesToCustftp
 
-----------------------================================>

# EZ-COM notifications are CSV files delivered via FTP
# (currently to ossbilling1.dal01:/home/ezcom/notifications)

import os, sys, re, csv

from voex.native import ServiceError

from cdrPublishingService.errors import *


class EzcomNotification(object):
    def __init__(self):
        self.path = None
        self._rows = []


    def add_row(self,row):
        self._rows.append(row)

        
    def rows(self):
        #for i in self._rows:
        #    yield i
        return self._rows
            

    def count(self):
        return len(self._rows)
    
    
        
class InvoiceDetailReport(EzcomNotification):
    """ Class for parsing IDR files from EZ-COM """

    FIELDNAMES_V1 = ['Aggregator ID',
                     'Customer Number',
                     'Customer Name',
                     'SAP ID',
                     'Invoice #',
                     'Total Call Amount',
                     'Invoice Amount',
                     'Bill Frequency',
                     'Last Invoice Date',
                     'R8 Call Amount',
                     'IP Call Amount',
                     'IL Call Amount',
                     '2P Call Amount',
                     'MRC',
                     'Taxes',
                     'Total Invoice Minutes']

    # Newer format (which should include headers by default) introduced
    # additional call types:
    FIELDNAMES = ['Aggregator',
                  'Account#',
                  'Name',
                  'SAP ID',
                  'Invoice#',
                  'Call Amount',
                  'Invoice$',
                  'Bill Freq',
                  'Inv Date',
                  'R8 Call Amt',
                  'IP Call Amt',
                  'IL Call Amt',
                  '2P Call Amt',
                  'R9 Call Amt',
                  '3P Call Amt',
                  'FF Call Amt',
                  'Charges',
                  'Taxes',
                  'Minutes']
    
    def __init__(self):
        super(InvoiceDetailReport,self).__init__()
        self.batch_name = None
        self.is_v1 = False

        self.index_account_nr = {}
        self.index_sap_id = {}
        self.index_name = {}
        self.index_invoice_nr = {}
        
    def parse_file(self,path):
        self.path = path
        fh = open(path,'rU')
        self.parse(fh)

        self.set_batch_name_from_path(path)

    
    def parse(self,fh):
        # Determine the CSV dialect, and whether headers are included
        # The CSV from EzCom doesn't currently contain field names, but it may
        # in the future
        sample = fh.read(5000)
        fh.seek(0)
        
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(sample)

        fieldnames = None
        #if not sniffer.has_header(sample): # sniffer header detection not accurate...
        if not 'Aggregator' in sample:
            # If the IDR doesn't have a header row, it is most likely a V1 IDR:
            fieldnames = self.FIELDNAMES_V1
            self.is_v1 = True

        reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
        for row in reader:
            for k in row.keys():
                row[k] = row[k].strip()
            self.add_row(row)

        if self.is_v1:
            self.fix_v1_to_current()

        self.index()
        

    def fix_v1_to_current(self):
        """ Translate column names from V1 format to latest format, and add missing columns """
        for row in self.rows():
            row['Aggregator'] = row['Aggregator ID']
            row['Account#'] = row['Customer Number']
            row['Name'] = row['Customer Name']
            #row['SAP ID'] = row['SAP ID']
            row['Invoice#'] = row['Invoice #']
            row['Call Amount'] = row['Total Call Amount']
            row['Invoice$'] = row['Invoice Amount']
            row['Bill Freq'] = row['Bill Frequency']
            row['Inv Date'] = row['Last Invoice Date']
            row['R8 Call Amt'] = row['R8 Call Amount']
            row['IP Call Amt'] = row['IP Call Amount']
            row['IL Call Amt'] = row['IL Call Amount']
            row['2P Call Amt'] = row['2P Call Amount']
            row['R9 Call Amt'] = '0.0'
            row['3P Call Amt'] = '0.0'
            row['FF Call Amt'] = '0.0'
            row['Charges'] = row['MRC']
            #row['Taxes'] = row['Taxes']
            row['Minutes'] = row['Total Invoice Minutes']
            
            

    def set_batch_name(self,batch_name):
        self.batch_name = batch_name

        # Also add to any of the individual rows that have been loaded:
        for inv in self.rows():
            inv['Batch Name'] = batch_name


    def set_batch_name_from_path(self,path):
        batch_name, ext = os.path.splitext(os.path.basename(path))
        self.set_batch_name(batch_name)


    def index(self):
        """ Populate the indices """
        for inv in self.rows():
            self.index_account_nr[inv['Account#']] = inv
            self.index_sap_id[inv['SAP ID']] = inv
            self.index_name[inv['Name']] = inv
            self.index_invoice_nr[inv['Invoice#']] = inv


    def find_by_name(self,name):
        try: return self.index_name[name]
        except KeyError: return None

    def find_by_account_nr(self,account):
        try: return self.index_account_nr[account]
        except KeyError: return None

        
    class Supplemental(EzcomNotification):
        """ Supplemental invoice detail report (SDR) that goes along with an IDR.
            Each row of the SDR gives total calls, mou, and charges per call-type / sub-call-type.
            The IDR gives just the total amount per Call-type, so this is the detail underneath
            that """
        FIELDNAMES = ['Aggregator',
                      'Account#',
                      'Name',
                      'SAP ID',
                      'Invoice#',
                      'Call Type',
                      'Sub-Call Type',
                      'Total Calls',
                      'Total Minutes',
                      'Total Amount']

        def __init__(self,idr=None):
            super(InvoiceDetailReport.Supplemental,self).__init__()
            self.idr = idr

        def parse_file(self,path=None):
            if not path:
                path = self.determine_path_from_idr()
            self.path = path
            if os.path.exists(self.path):
                fh = open(path,'rU')
                self.parse(fh)
            elif self.idr.is_v1:
                # No SDR for old IDR formats
                pass
            else:
                raise ServiceError(Errors.MissingSDR,self.path)


        def determine_path_from_idr(self):
            """ Determine the filename to load given the IDR this SDR is tied to.
                For an IDR name 'IDR_xxx', the SDR will be 'SDR_xxx' """
            if self.idr and self.idr.path:
                path = self.idr.path
                dir = os.path.dirname(path)
                base = os.path.basename(path)
                if base.startswith('IDR_'):
                    return os.path.join(dir,'SDR_' + base[4:])
            raise ServiceError(Errors.MissingSDR,self.idr.path if self.idr else None)

    
        def parse(self,fh):
            # Determine the CSV dialect, and whether headers are included
            sample = fh.read(5000)
            fh.seek(0)
        
            sniffer = csv.Sniffer()
            dialect = sniffer.sniff(sample)

            fieldnames = None
            #if not sniffer.has_header(sample): # sniffer header detection not accurate...
            if not 'Aggregator' in sample:
                fieldnames = self.FIELDNAMES
            
            reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
            for row in reader:
                for k in row.keys():
                    row[k] = row[k].strip()
                self.add_row(row)

            if self.idr:
                self.associate_idr()
                self.summarize_call_types()
        

        def associate_idr(self):
            """ Match up the detail rows from this file with the higher-level summary from the IDR """
            for row in self.rows():
                self.associate_idr_row(row)


        def associate_idr_row(self,row):

            #import pdb; pdb.set_trace()

            inv = self.idr.find_by_name(row['Name'])
            if not inv:
                return

            row['Invoice'] = inv
            
            call_type = row['Call Type']
            sub_type = row['Sub-Call Type']

            print 'Invoice'


            if '%s Detail' % call_type in inv and sub_type in inv['%s Detail' % call_type]:
                #print "Pair Exists"
                totalAmount = row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Amount']
                totalCalls = row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Calls']
                totalMinutes = row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Minutes']
            else:
                #print "Pair Does Not exist"
                totalAmount = 0
                totalCalls = 0
                totalMinutes = 0


            if '%s Detail' % (call_type) in inv:
                print "Detail exists"
                detail = inv['%s Detail' % (call_type)]
            else:
                print "Detail doesn't exist"
                detail = inv['%s Detail' % (call_type)] = {}

            detail[row['Sub-Call Type']] = row

            row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Amount'] = totalAmount + float(row['Total Amount'])
            row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Calls'] = totalCalls + float(row['Total Calls'])
            row['Invoice']['%s Detail' % (call_type)][sub_type]['Total Minutes'] = totalMinutes + float(row['Total Minutes'])

            #import pprint
            #pp = pprint.PrettyPrinter(indent=4)
            #pp.pprint(row)


        def summarize_call_types(self):
            """ The IDR gives total amount per call type. This method also adds
                total call count, and total mou, by summing up the sub-call types
                per call type """
            for row in self.rows():
                self.summarize_call_types_row(row)


        def summarize_call_types_row(self,row):
            try: inv = row['Invoice']
            except KeyError: return

            call_type = row['Call Type']
            sub_type = row['Sub-Call Type']

            row_calls = int(row['Total Calls'])
            try: inv['%s Call Count' % (call_type)] += row_calls
            except KeyError: inv['%s Call Count' % (call_type)] = row_calls
            
            row_mou = float(row['Total Minutes'])
            try: inv['%s Call Minutes' % (call_type)] += row_mou
            except KeyError: inv['%s Call Minutes' % (call_type)] = row_mou



class AccountList(EzcomNotification):
    """ Class for parsing AL files from EZ-COM """

    FIELDNAMES = ['Aggregator ID',
                  'Customer Number',
                  'Customer Name',
                  'SAP ID',
                  'Bill Frequency',
                  'Email']
    
    def parse_file(self,path):
        fh = open(path,'rU')
        self.parse(fh)

    def parse(self,fh):
        # Determine the CSV dialect, and whether headers are included
        # The CSV from EzCom doesn't currently contain field names, but it may
        # in the future
        sample = fh.read(5000)
        fh.seek(0)
        
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(sample)

        fieldnames = None
        #if not sniffer.has_header(sample): # sniffer header detection not accurate...
        if not 'Customer Name' in sample:
            fieldnames = self.FIELDNAMES

        reader = csv.DictReader(fh, dialect=dialect, fieldnames=fieldnames)
        for row in reader:
            for k in row.keys():
                row[k] = row[k].strip()
            self.add_row(row)
            
-----------------------================================>
    # This is the bit you want for doing a find, Julius
    dd =  { 'billrec.num.called': 1,
        'billrec.num.calling': 1,
        'udr.start.date': 1,
        'udr.start.time': 1,
        'udr.disc.date': 1,
        'udr.disc.time': 1,
        'udr.ani.nat': 1,
        'udr.lrn.nat': 1,
        'udr.ip.orig': 1,
        'udr.ip.term': 1,
        '_id': 0 }
    #curs = cl.merged_cdrs.merged_cdrs_20140806.find({'udr.ani.nat':"6172792393"},dd.keys())
    curs = c.find( { '$or' : [ {'billrec.num.called':'subscriber'}, {'billrec.num.calling':'subscriber'}]},dd.keys())

    with open('mycsvfile.csv','wb') as f:
        for item in curs:
            w = csv.writer(f)
            w.writerow(item.keys())
            w.writerow(item.values())
            w.writerow(str(item['udr']['ani']['nat']))
-----------------------================================>
#/bin/bash  -x

# 'fix_fidelity'

services="FIDELITY_VOICE_SERVICES_$(date +'%Y')_$(date +'%m')"
svcs_trz="FIDELITY_VOICE_SVCS_TRZ_$(date +'%Y')_$(date +'%m')"
data_trzr="FIDELITY_VOICE_AND_DATA_TRZR_$(date +'%Y')_$(date +'%m')"

echo $services
echo $svcs_trz
echo $data_trzr

if [ ! -d tmp ]; then
    echo "Directory tmp does not exist"
    mkdir tmp
fi

cd tmp
if [ $? -eq 0 ]; then
    echo "Changed directory to tmp"
    rm -rf *
else
    echo "Couldn't change directory to tmp. Exiting..."
    exit
fi

processFile()
{
    zipfile="../$1*.zip"
    echo "processFile $zipfile"
    if [ -f $zipfile ]; then
        echo "$zipfile exists" 
        cp $zipfile .
        unzip $1*.zip
        csv="$1*.csv"
        echo $csv
        /home/ageorge/bin/ensureDecimals.py $csv tmpfile.csv
        /home/ageorge/bin/unix2dos tmpfile.csv
        mv tmpfile.csv $csv
        #echo "zip $zipfile $csv"
        echo "zip $1*.zip $csv"
        conv_zipfile="$1*.zip"
        echo "conv_zipfile $conv_zipfile"
        zip $conv_zipfile $csv
        cp $conv_zipfile ..
        rm *.csv *.zip
    fi
}

processFile $services
processFile $svcs_trz
processFile $data_trzr

cd ..
rmdir tmp
-----------------------================================>
-----------------------================================>
#/bin/bash  -x

inputfile="./Data.txt"
# tr -d '\r' < Data.txt > datafile

cat $inputfile | while read TrunkGroupID TrunkGroupName GatewayID; do
   echo "INSERT into foo (COL1, COL2, COL3) VALUES ('$TrunkGroupID', '$TrunkGroupName', '$GatewayID');" >> output
done
-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv
import pickle
import csv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def cdrs(day, invoice, criteria):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match' : criteria},
        {'$group' : { '_id': 1, 'total' : {'$sum': '$rated.bill.amt'}}},
    ]

    res = c.aggregate(pipe)

    return res['result'][0]['total'] if len(res['result'])>0 else 0

def writeheader(writer):
    writer.writerow(dict((fn, fn) for fn in writer.fieldnames))


def run_search(month, subscriber):
    #import pdb; pdb.set_trace()

    file = open('results.csv', 'wb+')
    file.write('billrec.num.called' + ',')
    file.write('billrec.num.calling' + ',')
    file.write('udr.start.date' + ',')
    file.write('udr.start.time' + ',')
    file.write('udr.disc.date' + ',')
    file.write('udr.disc.time' + ',')
    file.write('udr.ani.nat' + ',')
    file.write('udr.lrn.nat' + ',')
    file.write('udr.ip.orig' + ',')
    file.write('udr.ip.term' + '\n')
    file.close()

    for day in month:
        get_day(day, subscriber)


def get_day(day, subscriber):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    #colname = 'merged_cdrs_20140806'
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match':{ '$or' : [ {'billrec.num.called':subscriber}, {'billrec.num.calling':subscriber}]}},
        {'$group':{ '_id':
                       {'billrec_num_called':'$billrec.num.called',
                       'billrec_num_calling':'$billrec.num.calling',
                       'udr_start_date':'$udr.start.date',
                       'udr_start_time':'$udr.start.time',
                       'udr_disc_date':'$udr.disc.date',
                       'udr_disc_time':'$udr.disc.time',
                       'udr_ani_nat':'$udr.ani.nat',
                       'udr_lrn_nat':'$udr.lrn.nat',
                       'udr_ip_orig':'$udr.ip.orig',
                       'udr_ip_term':'$udr.ip.term'
                       }}},
           ]
    #pipe.append({'$match':{'udr.ani.nat':"6172792393"}})
    docs = c.aggregate(pipe)
    #docs = cl.merged_cdrs.merged_cdrs_20140806.aggregate(pipe)
    #i = iter(docs)
    #item = i.next()
    #item = i.next()

    #itemCursor = cl.merged_cdrs.tmp_agg_res.find(exhaust=True)
    #for item in itemCursor:
        #print item

    #dd =  { 'billrec.num.called': 1,
        #'billrec.num.calling': 1,
        #'udr.start.date': 1,
        #'udr.start.time': 1,
        #'udr.disc.date': 1,
        #'udr.disc.time': 1,
        #'udr.ani.nat': 1,
        #'udr.lrn.nat': 1,
        #'udr.ip.orig': 1,
        #'udr.ip.term': 1,
        #'_id': 0 }
    #curs = cl.merged_cdrs.merged_cdrs_20140806.find({'udr.ani.nat':"6172792393"},dd.keys())
    ##curs = c.find({'udr.ani.nat':"6172792393"},dd.keys())
    #db.inventory.find( { $or: [ { quantity: { $lt: 20 } }, { price: 10 } ] } )
    #curs = c.find( { '$or' : [ {'billrec.num.called':'subscriber'}, {'billrec.num.calling':'subscriber'}]},dd.keys())

    #with open('mycsvfile.csv','wb') as f:
        #for item in curs:
            #w = csv.writer(f)
            #w.writerow(item.keys())
            #w.writerow(item.values())
            #w.writerow(str(item['udr']['ani']['nat']))

    file = open('results.csv', 'ab+')
    for item in docs['result']:
        file.write(str(item['_id']['billrec_num_called'])+ ',')
        file.write(str(item['_id']['billrec_num_calling']) + ',')
        file.write(str(item['_id']['udr_start_date']) + ',')
        file.write(str(item['_id']['udr_start_time']) + ',')
        file.write(str(item['_id']['udr_disc_date']) + ',')
        file.write(str(item['_id']['udr_disc_time']) + ',')
        file.write(str(item['_id']['udr_ani_nat']) + ',')
        file.write(str(item['_id']['udr_lrn_nat']) + ',')
        file.write(str(item['_id']['udr_ip_orig']) + ',')
        file.write(str(item['_id']['udr_ip_term']) + '\n')

    file.close()


if __name__ == '__main__':
    if len(argv) <= 3:
        print 'Usage: subpoena.py <start date yyyy-mm-dd> <end date yyyy-mm-dd> <subscriber#>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]
    subscriber = argv[3]

    run_search(month, subscriber)

-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def cdrs(day, invoice, criteria):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    c = cl.merged_cdrs[colname]

    pipe = [\
        {'$match' : criteria},
        {'$group' : { '_id': 1, 'total' : {'$sum': '$rated.bill.amt'}}},
    ]

    res = c.aggregate(pipe)

    return res['result'][0]['total'] if len(res['result'])>0 else 0


def hasUdr(day, invoice):
    criteria = {'rated.bill.inv': invoice, 'has.u' : {'$gt': 0}}
    return cdrs(day,invoice,criteria)

def hasBillrec(day, invoice):
    criteria = {'rated.bill.inv': invoice, 'has.b' : {'$gt': 0}}
    return cdrs(day,invoice,criteria)

def full_cdrs(day, invoice):
    criteria = {'has.b':{'$gt':0},
                'has.u':{'$gt':0},
                'has.r':{'$gt':0},
                'rated.bill.inv': invoice}
    return cdrs(day,invoice,criteria)


def summary(day, invoice):
    c = cl.summaries.ezcom_invoices 
    criteria = {'invoice': invoice}
    res = c.find_one(criteria)
    dayStr = day.strftime('%Y-%m-%d')
    return res['dates'][dayStr]['revenue'] if res is not None and dayStr in res['dates'] else 0

def run_invoice(month, invoice):
    columns = [summary, hasUdr, hasBillrec, full_cdrs]
    def valueStr(values):
        def eq(a,b):
            try:
                return fabs(a-b) < 0.1
            except:
                return a!=b

        res = ''
        prev = None
        for i in columns:
            val = values[i]
            res += ' %s %10s' % ('<>' if prev and not eq(val,prev) else '  ', val)
            prev = val
        return res

    columnNames = dict((x,x.__name__) for x in columns)
    totals = dict((x,0) for x in columns)

    print 'From %s to %s:' % (month[0], month[-1])
    print '%10s' % 'Day', valueStr(columnNames) 

    for day in month:
        amounts = dict((x,x(day,invoice)) for x in columns)
        for i in totals:
            totals[i] += amounts[i]
        print day.strftime('%Y-%m-%d'), valueStr(amounts)

    print 'Totals:', valueStr(totals)




if __name__ == '__main__':
    if len(argv) <= 3:
        print 'Usage: summary-discrepancy.py <start date yyyy-mm-dd> <end date yyyy-mm-dd> <invoice#>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]
    invoice = int(argv[3])

    run_invoice(month, invoice)

-----------------------================================>
#!/opt/anaconda/bin/python
import os
import sys
import optparse
import shutil
import csv
from collections import namedtuple

AttemptCsvComponents = [
            "derivedStartDate",
            "derivedStartTime",
            "derivedDisconnectTime",
            "mCustomerServiceName",
            "mAniNational",
            "mDestNational",
            "mDialedNational",
            "mDisconnectReason",
            "mDisconnectInitiator",
            "derivedDisconnectDate" ]
AttemptCsvLineComponents = namedtuple('AttemptCsvLineComponents', ' '.join(AttemptCsvComponents))


class StaticFieldTranslator:
    
    DisconnectReasonDict = {
                            '400': '41',
                            '401': '21',
                            '402': '21',
                            '403': '21',
                            '404': '1',
                            '405': '63',
                            '406': '79',
                            '407': '21',
                            '408': '102',
                            '410': '22',
                            '413': '127',
                            '414': '127',
                            '415': '79',
                            '416': '127',
                            '420': '127',
                            '421': '127',
                            '423': '127',
                            '480': '18',
                            '481': '41',
                            '482': '25',
                            '483': '25',
                            '484': '28',
                            '485': '1',
                            '486': '17',
                            '487': '16',
                            '500': '41',
                            '501': '79',
                            '502': '38',
                            '503': '41',
                            '504': '102',
                            '505': '127',
                            '513': '127',
                            '600': '17',
                            '603': '21',
                            '604': '1',
                            '1000': '16' ,
                            '1001': '41',
                            '1002': '41',
                            '1003': '41',
                            '1004': '41',
                            '1005': '102',
                            '1006': '63',
                            '1007': '47',
                            '1008': '34',
                            '1009': '41',
                            '1010': '102',
                            '1011': '102',
                            '1012': '16',
                            '1013': '41',
                            '1014': '41',
                            '1015': '44',
                            '1016': '42',
                            '1017': '42',
                            '1018': '34',
                            '1019': '34',
                            '1020': '34',
                            '1021': '63',
                            '1022': '41',
                            '1023': '16',
                            '1024': '102',
                            '1025': '102',
                            '1026': '102',
                            '1027': '41',
                            '1028': '3',
                            '1029': '41',
                            '1030': '96',
                            '1031': '42',
                            '1032': '42',
                            '1033': '21',
                            '1034': '41',
                            '1035': '41',
                            '1036': '41',
                            '1037': '57',
                            '1038': '34',
                            '1039': '41',
                            '1040': '42',
                            '1041': '42',
                            '1042': '42',
                            '1043': '34',
                            '1044': '102',
                            '1045': '63',
                            '1046': '63',
                            '1047': '16',
                            '1048': '102',
                            '1049': '96',
                            '1050': '47',
                            '1051': '3',
                            '1052': '102',
                            '1053': '44',
                            '1054': '41',
                            '1055': '41',
                            '1056': '101',
                            '1057': '101',
                            '1058': '110',
                            '1059': '34',
                            '1060': '34',
                            '1061': '34',
                            '1062': '34',
                            '1063': '34',
                            '1064': '41'
                            } 
    
    @staticmethod
    def translate_all(components):
        ''' call all translators
            each translator returns a new AttemptCsvLineComponents instance -- this is inefficient
            at the cost of the namedtuple benefits
        '''
        result = StaticFieldTranslator.translate_mDisconnectReason(components)
        return result
    
    @staticmethod
    def translate_mDisconnectReason(components):
        '''translate the mDisconnectReason field, if applicable'''
        new_val = StaticFieldTranslator.DisconnectReasonDict.get(components.mDisconnectReason, components.mDisconnectReason)
        if new_val == components.mDisconnectReason:
            return components
        else:
            return components._replace(mDisconnectReason=new_val)
        
        
            
def ParseOpts():
    '''Parse the command line options'''
    parser = optparse.OptionParser()
    
    parser.add_option("--inputfile", dest="inputfile", help="csv file to translate")
    parser.add_option("--outputfile", dest="outputfile", help="destination for resulting translated file")
        
    opts, args = parser.parse_args()
    return opts, args
    
    
def main():
    opts, args = ParseOpts()
    
    if opts.inputfile is None or opts.outputfile is None:
        print 'ERROR: empty arguments found[%s]' % (opts,)
    else:
        input_file_copy_filename = opts.inputfile + '.TMP'
        try:
            #operate on a copy of the file since the existing file might still be being written to
            shutil.copy(opts.inputfile, input_file_copy_filename)
            
            with open(input_file_copy_filename, 'rb') as inputcsvfile:
                csvreader = csv.reader(inputcsvfile)
                
                with open(opts.outputfile, 'w') as outputcsvfile:
                    outputcsvwriter = csv.writer(outputcsvfile, quoting=csv.QUOTE_ALL)
                    
                    for row in csvreader:
                        try:
                            attemptComponents = AttemptCsvLineComponents(*row)
                            attemptComponents = StaticFieldTranslator.translate_all(attemptComponents)
                            outputcsvwriter.writerow(attemptComponents)
                        except TypeError:
                            #print any parse errors, but continue regardless
                            print ('WARNING: invalid attempt line read into AttemptCsvLineComponents namedtuple:' +
                                '\n\t%s' +
                                '\n\tEither the file was not fully written, or an invalid file format is being read') % (row, )    
        except:
            print "Unexpected error [%s]" % (sys.exc_info()[0],)
            raise                            
        finally:
            if os.path.isfile(input_file_copy_filename):
                os.remove(input_file_copy_filename)
        
              

if __name__ == '__main__':
    main()
-----------------------================================>
#!/usr/bin/python

from pymongo import *
import pprint
from datetime import *
from math import fabs
from sys import argv
import pickle
import csv

pp = pprint.PrettyPrinter(indent=4)
cl = MongoClient('uccgen03.den02', 27017)

def run_search(month):
    #import pdb; pdb.set_trace()

    #
    # Set the output file name, and write the column headers.
    #
    file = open('julius.csv', 'wb+')
    file.write('udr.pkg' + ',')
    file.write('billrec.cust' + ',')
    file.write('udr.gw' + ',')
    file.write('udr.tg.out' + ',')
    file.write('udr.file' + ',')
    file.write('udr.vend.name' + ',')
    file.write('udr.cust.name' + ',')
    file.write('udr.dial.nat' + '\n')
    file.close()

    for day in month:
        get_day(day)


def get_day(day):
    colname = 'merged_cdrs_' + day.strftime('%Y%m%d')
    #colname = 'merged_cdrs_20140806'
    c = cl.merged_cdrs[colname]


    #
    # This is the data structure that maps to the result of your query.
    #
    dd = { 'udr.pkg':1,
        'billrec.cust':1,
        'udr.gw':1,
        'udr.tg.out':1,
        'udr.file':1,
        'udr.vend.name':1,
        'udr.cust.name':1,
        'udr.dial.nat':1 }

    #
    # This is the Mongo query.
    #
    curs = c.find({'udr.prod.code':"DD", 'udr.pkg':""}, dd.keys())

    #
    # You get a cursor back, and can just iterate through,
    # writing to the csv file.
    #
    file = open('julius.csv', 'ab+')
    for item in curs:
        file.write(str(item['udr']['pkg'])+ ',')
        file.write(str(item['billrec']['cust']) + ',')
        file.write(str(item['udr']['gw']) + ',')
        file.write(str(item['udr']['tg']['out']) + ',')
        file.write(str(item['udr']['file']) + ',')
        file.write(str(item['udr']['vend']['name']) + ',')
        file.write(str(item['udr']['cust']['name']) + ',')
        file.write(str(item['udr']['dial']['nat']) + '\n')

    file.close()


if __name__ == '__main__':
    if len(argv) <= 2:
        print 'Usage: udd.py <start date yyyy-mm-dd> <end date yyyy-mm-dd>'
        exit(0)

    startDate = datetime.strptime(argv[1], '%Y-%m-%d')
    endDate = datetime.strptime(argv[2], '%Y-%m-%d')
    nDays = (endDate - startDate).days + 1
    month = [startDate + timedelta(days=x) for x in xrange(nDays)]

    run_search(month)

-----------------------================================>
